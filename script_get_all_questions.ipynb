{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case Cleaning\n",
    "\n",
    "* First, we copied all case files from [Oyez's Github Repo](https://github.com/walkerdb/supreme_court_transcripts)\n",
    "* Then, we delete all non-transcript files (files that don't end with \"t01\" or \"t02\")\n",
    "* Then, we remove all files from 2024\n",
    "* Next, we remove all files that don't contain a non-null \"transcript\" field\n",
    "* Finally, we remove all files with only one section in the transcript field\n",
    "\n",
    "Text Cleaning\n",
    "* We consolidate all the text within a Scotus Justice's turn and keep text that has at least one \"?\" inside the text.\n",
    "* we omit lines < 50 characters because they will not help with contextual embeddings.(For example, a common question justices ask is \"why?\" but it noise in our vector embedding without the surrounding context. -- This is a heuristic approach that we might revisit)\n",
    "* Around ~8000 lines had some \"Inaudible\" strings in them, so we remove these lines as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# initialize script variables\n",
    "all_questions_list = []\n",
    "\n",
    "def add_questions_from_section(section_json, json_file_name, question_addressee):\n",
    "    '''\n",
    "    Add all questions from transcript section into question list (global var).\n",
    "    Each \"question\" contains metadata of the transcript_id, to whom the question is addressed,\n",
    "    the Supreme Court Justice speaking, and the question text.\n",
    "    '''\n",
    "    turns = section_json[\"turns\"]\n",
    "    for turn in turns:\n",
    "        # 1) check for speaker not existing, 2) inaudible audio, or 3) Elizabeth Prelogar\n",
    "        if turn[\"speaker\"] == None or turn[\"speaker\"][\"roles\"] == None or len(turn[\"speaker\"][\"roles\"]) == 0:\n",
    "            continue\n",
    "        \n",
    "        # check for 1) Justice Amy Coney Barrett or 2) all other justices\n",
    "        if ('2' in turn[\"speaker\"][\"roles\"] and turn[\"speaker\"][\"roles\"]['2'][\"type\"] == \"scotus_justice\") or turn[\"speaker\"][\"roles\"][0][\"type\"] == \"scotus_justice\":\n",
    "            all_text_in_turn =  \" \".join([block[\"text\"] for block in turn[\"text_blocks\"]])\n",
    "            \n",
    "            # check for questions in text turn; use question len > 50 chars as heuristic\n",
    "            if len(all_text_in_turn) > 50 and \"?\" in all_text_in_turn and \"inaudible\" not in all_text_in_turn.lower():\n",
    "                question_info = {}\n",
    "                question_info[\"transcript_id\"] = json_file_name[:-5]\n",
    "                question_info[\"question_addressee\"] = question_addressee\n",
    "                question_info[\"justice\"] = turn[\"speaker\"][\"name\"]\n",
    "                question_info[\"question_text\"] = all_text_in_turn\n",
    "\n",
    "                all_questions_list.append(question_info)\n",
    "\n",
    "def add_questions_from_transcript(json_file_name):\n",
    "    '''\n",
    "    Given a transcript, add all justice questions satisfying the criteria in the \n",
    "    data cleaning notes above to our global question list.\n",
    "    '''\n",
    "    with open(TRANSCRIPTS_DIR + json_file_name) as json_file:\n",
    "        transcript_json = json.load(json_file)\n",
    "\n",
    "    sections = transcript_json[\"transcript\"][\"sections\"]\n",
    "    for i in range(len(sections)):\n",
    "        if i % 2 == 0:\n",
    "            question_addressee = \"petitioner\"\n",
    "        else:\n",
    "            question_addressee = \"respondent\"\n",
    "\n",
    "        add_questions_from_section(sections[i], json_file_name, question_addressee)\n",
    "\n",
    "'''\n",
    "Main function --> goes through all transcripts and add questions to the global list.\n",
    "'''\n",
    "TRANSCRIPTS_DIR = \"transcripts_up_to_2024/\"\n",
    "FILE_TO_SAVE = \"all_questions.csv\"\n",
    "\n",
    "for json_file_name in os.listdir(TRANSCRIPTS_DIR):\n",
    "    # ignore hidden files\n",
    "    if json_file_name.startswith('.'):\n",
    "        continue\n",
    "    add_questions_from_transcript(json_file_name)\n",
    "\n",
    "questions_df = pd.DataFrame(all_questions_list)\n",
    "questions_df.to_csv(FILE_TO_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oral-arg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
