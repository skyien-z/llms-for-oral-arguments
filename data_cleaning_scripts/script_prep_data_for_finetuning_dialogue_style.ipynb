{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71670f7-c11a-4be5-a795-4f53f4289ff4",
   "metadata": {},
   "source": [
    "# Prep Data for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8563250-503e-4b80-aeea-15dc9c17c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "\n",
    "TRANSCRIPTS_DIR = \"../transcripts_up_to_2024/\"      # directory of raw JSONs of oral arguments\n",
    "OUT_DIR = \"../datasets/finetune\"\n",
    "\n",
    "def save_jsonl(df, filename):\n",
    "    df.to_json(filename, orient=\"records\", lines=True)\n",
    "\n",
    "def read_jsonl(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc692cb8-f997-4b50-b48b-0586d35b37c8",
   "metadata": {},
   "source": [
    "## Prep training samples for finetuning in the following format:\n",
    "```\n",
    "training_samples = [\n",
    "    {\n",
    "        \"transcript_id\": \"2022.21-376-t01\",\n",
    "        \"chunk_id\": 0,\n",
    "        \"content\": [\n",
    "            {\"role\": \"advocate\", \"content\": \"Advocate gives their opening statement\"},\n",
    "            {\"role\": \"justice_sonia_sotomayor\", \"content\": \"Sotomayor asks a question based on advocate's opening statement.\"},\n",
    "            {\"role\": \"advocate\", \"content\": \"Advocate says something in response to Sotomayor\"},\n",
    "            {\"role\": \"justice_samuel_a_alito_jr\", \"content\": \"Something Justice Alito said in transcript.\"},\n",
    "        ]\n",
    "    },\n",
    "...\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "819079e5-6833-4bfc-b1c5-af925c743e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARS = 5000  # Maximum characters per chunk\n",
    "\n",
    "def get_formatted_text_of_turn(turn):\n",
    "    '''\n",
    "    Return all text within a turn as a dict denoting speaker, role, and text.\n",
    "\n",
    "    @param turn -- JSON representing a single speaker turn\n",
    "    @return -- Dict with keys \"role\", \"content\"\n",
    "    '''\n",
    "    if not turn[\"speaker\"]:  # skip turns that have no speaker like \"Laughter\"\n",
    "        return None\n",
    "\n",
    "    if not turn[\"speaker\"][\"roles\"]:\n",
    "        role = \"attorney\"\n",
    "    elif ('2' in turn[\"speaker\"][\"roles\"] and turn[\"speaker\"][\"roles\"]['2'][\"type\"] == \"scotus_justice\") or \\\n",
    "         turn[\"speaker\"][\"roles\"][0][\"type\"] == \"scotus_justice\":\n",
    "        role = \"scotus_justice\"\n",
    "\n",
    "    if role == \"scotus_justice\":\n",
    "        identifier = f'justice_{turn[\"speaker\"][\"identifier\"]}'\n",
    "    else:\n",
    "        identifier = \"advocate\"\n",
    "\n",
    "    text = \" \".join([block[\"text\"] for block in turn[\"text_blocks\"]])\n",
    "\n",
    "    return {\n",
    "        \"role\": identifier,\n",
    "        \"content\": text\n",
    "    }\n",
    "\n",
    "def chunk_transcript_content(transcript_id, formatted_turns, max_chars=MAX_CHARS):\n",
    "    '''\n",
    "    Chunks formatted turns into samples using a sliding window\n",
    "\n",
    "    @param transcript_id -- The ID of the transcript\n",
    "    @param formatted_turns -- List of formatted speaker turns\n",
    "    @param max_chars -- Maximum characters allowed per chunk\n",
    "    @return -- List of chunked transcript segments\n",
    "    '''\n",
    "    chunks = []\n",
    "    current_chunk = deque()\n",
    "    current_length = 0\n",
    "    chunk_id = 0\n",
    "\n",
    "    for turn in formatted_turns:\n",
    "        turn_text = turn[\"content\"]\n",
    "        turn_length = len(turn_text)\n",
    "\n",
    "        # If adding this turn exceeds max_chars, remove old turns from the front\n",
    "        while current_length + turn_length > max_chars and current_chunk:\n",
    "            removed_turn = current_chunk.popleft()\n",
    "            current_length -= len(removed_turn[\"content\"])\n",
    "\n",
    "        # Add new turn\n",
    "        current_chunk.append(turn)\n",
    "        current_length += turn_length\n",
    "\n",
    "        # Save current window as a chunk\n",
    "        chunks.append({\n",
    "            \"transcript_id\": transcript_id,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"content\": list(current_chunk)\n",
    "        })\n",
    "        chunk_id += 1\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def get_transcript_data(json_file_name):\n",
    "    '''\n",
    "    @param json_file_name -- Name of the oral argument JSON file\n",
    "    @return -- List of chunked transcript samples\n",
    "    '''\n",
    "\n",
    "    transcript_file_path = os.path.join(TRANSCRIPTS_DIR, json_file_name)\n",
    "    with open(transcript_file_path, 'r') as json_file:\n",
    "        transcript_json = json.load(json_file)\n",
    "\n",
    "    transcript_id = json_file_name[:-5]\n",
    "    formatted_turns = []\n",
    "\n",
    "    for section in [0, 1]:\n",
    "        section_turns = transcript_json[\"transcript\"][\"sections\"][section][\"turns\"]\n",
    "        section_turns = [get_formatted_text_of_turn(turn) for turn in section_turns]\n",
    "        section_turns = [turn for turn in section_turns if turn]  # remove None values\n",
    "        formatted_turns.extend(section_turns)\n",
    "\n",
    "    return chunk_transcript_content(transcript_id, formatted_turns, MAX_CHARS)\n",
    "\n",
    "data_transcripts = []\n",
    "cases_dir = os.fsencode(TRANSCRIPTS_DIR)\n",
    "for json_file_name in os.listdir(TRANSCRIPTS_DIR):\n",
    "    if json_file_name.endswith('.json'):\n",
    "        data_transcripts.extend(get_transcript_data(json_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23fb5578-d863-44dd-b403-f2871818335f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455031"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df0083a-bd50-40e0-81a5-536d823784ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transcript_id': '2009.08-1008-t01',\n",
       " 'chunk_id': 164,\n",
       " 'content': [{'role': 'advocate',\n",
       "   'content': \"--Well, Your Honor, I think that would raise some interesting questions about New York's power to--\"},\n",
       "  {'role': 'justice_john_g_roberts_jr',\n",
       "   'content': 'What it would do, it seems to me, is make it clear that was not a substantive decision, but, instead, a procedural decision.'},\n",
       "  {'role': 'advocate',\n",
       "   'content': \"--Correct, Your Honor. That's right. And, again -- and, again--\"},\n",
       "  {'role': 'justice_ruth_bader_ginsburg',\n",
       "   'content': \"But it could be -- it could be, as I -- the example of the statute of limitations. We create a claim. It has a certain life. It's dead after that time. That's New York law. A sister State may say, we create the same claim, but we think it has a longer life. New York would say, that's fine. Bring that claim in your own State. Don't clutter up our courts with out-of-State claims when we would not hear the identical claim under our own law. There are policies that do operate as procedural limitations and have a substantive thrust.\"},\n",
       "  {'role': 'advocate', 'content': '--Absolutely.'},\n",
       "  {'role': 'justice_ruth_bader_ginsburg',\n",
       "   'content': \"New York might well say, look, we don't hear in New York penalty cases, and so we are not going to entertain the sister State claim for any -- when we wouldn't entertain our own. We are not frustrating the sister State. They could bring the class action there, but not in -- not in our courts.\"},\n",
       "  {'role': 'advocate',\n",
       "   'content': \"And I think the point -- I agree 100 percent. I think the point that you are -- that point underscores, Your Honor, is that, ultimately, the Erie issue is a Federal issue. You can look to New York to try to understand the design and operation of the State rule at issue, but, ultimately, you are being asked, as a Federal court, to set the appropriate relationship between the State court system and the Federal court system. And, again, the lesson of Erie is you don't want to create incentives that will bring people like a magnet to Federal court and distort these ex ante foreign choices of litigants for State law claims.\"},\n",
       "  {'role': 'justice_ruth_bader_ginsburg',\n",
       "   'content': 'Well, they -- they bring up the Class Action Fairness Act, which allows a plaintiff -- they allow a defendant to remove a class action from a State court to a Federal court, but they also allow a plaintiff to initiate an action in the Federal court.'},\n",
       "  {'role': 'advocate',\n",
       "   'content': \"That's correct, Your Honor, but the Class Action Fairness Act, on its face -- and the legislative history actually makes this point explicit -- it had no intention to change the operation of the Erie doctrine in class actions. And so there is nothing in the Class Action Fairness Act that changes the scope of Rule 23. Again, Rule 23 just doesn't address this antecedent issue. It assumes, but does not require, that you have a cause of action that is amenable to class certification in the first place. And if you were to construe Rule 23 otherwise, as overriding this kind of statute -- all the statutes in Appendix B, that would be a truly remarkably substantive interpretation that this Court has always stressed, that it must, in construing the rules, be careful not to tread into that territory and has construed the rules with an eye towards the limitations of the Rules Enabling Act. The other side -- Shady Grove would walk you right into an extremely problematic situation from the point of view of the Rules Enabling Act, as well as creating these -- these incentives that really go against the heart of the Erie doctrine that would turn a $500 case into a $5 million case. And one interesting point, I think, is that all these statutes that are listed in our Appendix B that limit class certification for particular causes of action -- under their theory that Rule 23 requires that everything be amenable to class certification, those would all be out the window. I don't think counsel really wanted to admit that this morning, but the logic of their theory that -- is that Rule 23 governs this case and Rule 23 requires that every cause of action that comes before it be eligible for class certification. That would knock out each and every one of the statutes in Appendix B. They don't live up to -- in their reply brief, at footnote 10, on page 15, they try to distinguish those statutes by saying, ah, well, the limitation on class actions in those statutes is in the substantive cause of action. It's not in -- it's not somewhere else in the code, but that doesn't -- that doesn't save their argument under Rule 23. They really can't square that with their -- their core position that Rule 23 itself answers the question presented in this case. And, again, what we would ask the Court is just to -- is to recognize that Rule 23 occupies the ground it occupies, but it doesn't go -- it occupies the ground of the criteria, which go to the efficiency and fairness of the process. But where a State has made an antecedent decision that -- that a particular cause of action or a particular remedy is categorically unavailable -- or ineligible for class certification, that's a decision that Federal courts should respect under the Erie doctrine. If there are no further questions, I see my time is about to expire.\"},\n",
       "  {'role': 'justice_john_g_roberts_jr', 'content': 'Thank you, Mr. Landau.'},\n",
       "  {'role': 'advocate', 'content': 'Thank you, Chief Justice.'},\n",
       "  {'role': 'justice_john_g_roberts_jr',\n",
       "   'content': 'Mr. Nelson, you have 4 minutes remaining.'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transcripts[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce786b54-823b-414f-affc-af65724bb9bd",
   "metadata": {},
   "source": [
    "**Question**: Should we do any cleaning on these like filtering out turns that are inaudible/too-short? I was initially thinking yes, but maybe it's okay?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15faf960-b278-4580-87a1-e1ca683ed5dc",
   "metadata": {},
   "source": [
    "## Sanity test on chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0877a-c46d-4bcd-9f85-b9614a3a9830",
   "metadata": {},
   "source": [
    "Sample input json:\n",
    "\n",
    "```\n",
    "chat = [\n",
    "    {\"role\": \"advocate\", \"content\": \"Advocate gives their opening statement\"},\n",
    "    {\"role\": \"justice_sonia_sotomayor\", \"content\": \"Sotomayor asks a question based on advocate's opening statement.\"},\n",
    "    {\"role\": \"advocate\", \"content\": \"Advocate says something in response to Sotomayor\"},\n",
    "    {\"role\": \"justice_samuel_a_alito_jr\", \"content\": \"Something Justice Alito said in transcript.\"},\n",
    "]\n",
    "```\n",
    "\n",
    "This should map to something like the following after applying chat template (added new-lines for readability)\n",
    "```\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>\n",
    "advocate\n",
    "<|end_header_id|>\n",
    "Advocate gives their opening statement\n",
    "<|eot_id|>\n",
    "<|start_header_id|>\n",
    "justice_sonia_sotomayor\n",
    "<|end_header_id|>\n",
    "Sotomayor asks a question based on advocate's opening statement.\n",
    "<|eot_id|>\n",
    "<|start_header_id|>\n",
    "advocate\n",
    "<|end_header_id|>\n",
    "Advocate says something in response to Sotomayor\n",
    "<|eot_id|>\n",
    "<|start_header_id|>\n",
    "justice_samuel_a_alito_jr\n",
    "<|end_header_id|>\n",
    "Something Justice Alito said in transcript.\n",
    "<|eot_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b8ef8-e6c9-4339-974b-85810a616663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8c36d2-f655-43c8-8b6f-81152ce38604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.7: Fast Llama patching. Transformers: 4.48.1.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe MIG 1g.10gb. Max memory: 9.5 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"Llama-3.3-70B-Instruct-bnb-4bit\"\n",
    "MODEL_NAME = \"Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "# MODEL_NAME = \"Qwen2.5-32B-bnb-4bit\"\n",
    "\n",
    "model_name = f\"/scratch/gpfs/nnadeem/transformer_cache/{MODEL_NAME}/\"\n",
    "max_seq_length = 65536\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72159543-48e0-49b3-96bc-5afd993a37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_chat_template():\n",
    "    return \"\"\"<|begin_of_text|>{%- for message in messages %}<|start_header_id|>{{ message['role'] }}<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>{%- endfor %}\"\"\"\n",
    "tokenizer.chat_template = set_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dae54e2-a58a-42e3-85a3-8881bc99c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"advocate\", \"content\": \"Advocate gives their opening statement\"},\n",
    "    {\"role\": \"justice_sonia_sotomayor\", \"content\": \"Sotomayor asks a question based on advocate's opening statement.\"},\n",
    "    {\"role\": \"advocate\", \"content\": \"Advocate says something in response to Sotomayor\"},\n",
    "    {\"role\": \"justice_samuel_a_alito_jr\", \"content\": \"Something Justice Alito said in transcript.\"},\n",
    "]\n",
    "\n",
    "templated_chat = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "# templated_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc01cdf-f1d7-49e0-b23c-6e9fb2437b59",
   "metadata": {},
   "source": [
    "**Note**: Rather than using the custom `set_chat_template()` function defined above, we could alternatively modify the original chat template function of the model (modified to not include a system prompt with today's data always by default). Right now I just used a simpler template for clarity.\n",
    "\n",
    "For reference, original chat template for `unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit` is:\n",
    "```\n",
    "{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_finetuning_env [~/.conda/envs/llama_finetuning_env/]",
   "language": "python",
   "name": "conda_llama_finetuning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
