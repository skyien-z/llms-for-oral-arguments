{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Process Raw Data: Parse original Oyez transcript jsonls and into processed jsonl. Each line has the following keys: `{\"system_prompt\",\"instruction\", \"output\", \"transcript_id\", \"justice\"}`. **NOTE**: Optionally save to `finetuning_data_processed.jsonl`. Currently commented out this code to save on gitlfs storage.\n",
    "1. Filter Processed Data: \n",
    "    1. Filter out non-question samples, i.e. where the response > 50 chars and has \"?\" in it and does not have \"inaudible\" in it.\n",
    "    1. Filter out samples where the responding justice is not one of the current Supreme Court justices\n",
    "1. Train/Validation/Test Split: Split in 80/10/10 ratio and produce `train.jsonl`, `val.jsonl`, and `test.jsonl`\n",
    "\n",
    "**NOTE:** All output files are stored in `datasets/finetune/` and tracked with GIT LFS\n",
    "\n",
    "##### Sample from `test.jsonl`\n",
    "```\n",
    "{\n",
    "    \"system_prompt\":(\n",
    "        \"You are a Supreme Court Justice participating in oral arguments. \"\n",
    "        \"Given a transcript excerpt and a Justice's name, generate the Justice's next question in response to the conversation history.\"\n",
    "    ),\n",
    "    \"instruction\":(\n",
    "        \"<context>\\n\"\n",
    "        \"<turn>Jameson R. Jones: Mr. Chief Justice, and may it please the Court: As some of this questioning indicated, if any party has standing under Section 43(a) of the Lanham Act, it's a party whose goods are misrepresented in false advertising. To remove any doubt about that question, Congress amended the statute in 1988 to ensure a cause of action when a false advertiser misrepresents the goods or commercial services of, quote, \\u2036 another person \\u2033, end quote. This Court's zone of interest analysis shows that parties whose goods are disparaged, either expressly or by necessary implication, must have standing to sue. Lexmark's simply wrong about the idea that the zone of interest analysis in the Lanham Act does not impose limits upon who may sue. As the hypothetical with respect to the Bailey's Ice Cream Parlor shows, you can look to the subject matter of the false advertisement to see whose goodwill and commercial activities are related to the falsity of the statement. And those who come within the falsity and the subject matter of the advertisement at issue should have standing, while those who may have tangential injuries would not.<\\/turn>\\n\"\n",
    "        \"<turn>Justice Antonin Scalia: How do you -- how do you square that with the statutory provision that the purpose of the law is to prevent unfair competition? Unfair competition, not unfair trade practices? Unfair competition?<\\/turn>\\n\"\n",
    "        <turn>Jameson R. Jones: Where Section 45 says that it is designed to protect those engaged in such commerce from unfair competition, it's referring to what is defined in the operative text as unfair trade practices. Unfair competition involves specific measures, the use of falsities, that can injure parties who are not necessarily in competition with one another. The courts as a whole all agree that a competition requirement cannot be inferred into the false association cause of action that is also unfair competition that's part of Section 43(a). Section 43(a) goes to commercial activity. There is unfair competition in the sense that all of the activity under it is commercial and competitive in that sense. But some narrow form of competition between a plaintiff and a defendant for the purposes of standing is inconsistent with the structure of Section 43(a) and the text of the operative paragraph.<\\/turn>\\n\"\n",
    "        \"<\\/context>\\n\"\n",
    "        \"<justice>Justice Samuel A. Alito, Jr.<\\/justice>\\n\"\n",
    "        \"Generate a question that Justice Samuel A. Alito, Jr. is likely to ask next.\"\n",
    "    ),\n",
    "    \"output\": (\n",
    "        \"Justice Samuel A. Alito, Jr.: Suppose the comments in this case only disparaged the cartridges themselves and not the chips. \"\n",
    "        \"Then would the chip manufacturer, would your client have standing?\"\n",
    "    ),\n",
    "    \"transcript_id\":\"2013.12-873-t01\",\n",
    "    \"justice\":\"Justice Samuel A. Alito, Jr.\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TRANSCRIPTS_DIR = \"../transcripts_up_to_2024/\"      # directory of raw JSONs of oral arguments\n",
    "OUT_DIR = \"../datasets/finetune\"\n",
    "\n",
    "def save_jsonl(df, filename):\n",
    "    df.to_json(filename, orient=\"records\", lines=True)\n",
    "\n",
    "def read_jsonl(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Process Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 230841 fine-tuning examples to ../datasets/finetune/finetuning_data_processed.jsonl\n"
     ]
    }
   ],
   "source": [
    "def get_formatted_text_of_turn(turn):\n",
    "    '''\n",
    "    Return all text within a turn as a dict denoting speaker, role and text.\n",
    "    \n",
    "    @param turn -- JSON representing a single speaker turn\n",
    "    @return -- Dict with keys \"speaker_name\", \"role\", \"text\"\n",
    "    '''\n",
    "    if not turn[\"speaker\"]: # Skip turns that have no speaker like \"Laughter\"\n",
    "        return None\n",
    "    \n",
    "    if not turn[\"speaker\"][\"roles\"]:\n",
    "        role = \"attorney\"\n",
    "    # check for Justice Amy Coney Barrett (formatted with the roles['2']) and otherwise justices with  roles[0]\n",
    "    elif ('2' in turn[\"speaker\"][\"roles\"] and turn[\"speaker\"][\"roles\"]['2'][\"type\"] == \"scotus_justice\") or turn[\"speaker\"][\"roles\"][0][\"type\"] == \"scotus_justice\":\n",
    "        role = \"scotus_justice\"\n",
    "    \n",
    "    if role == \"scotus_justice\":\n",
    "        name = f\"Justice {turn[\"speaker\"][\"name\"]}\"\n",
    "    else:\n",
    "        name = turn[\"speaker\"][\"name\"]\n",
    "\n",
    "    text = \" \".join([block[\"text\"] for block in turn[\"text_blocks\"]])\n",
    "\n",
    "    formatted_turn = {\n",
    "        \"speaker_name\": name,\n",
    "        \"role\": role,\n",
    "        \"text\": text,\n",
    "    }\n",
    "\n",
    "    return formatted_turn\n",
    "\n",
    "def format_conversation_segment(context_turns, justice_turn, transcript_id):\n",
    "    '''\n",
    "        Formats conversation context and the justice's response into fine-tuning format.\n",
    "    '''\n",
    "\n",
    "    justice_name = justice_turn[\"speaker_name\"]\n",
    "    \n",
    "    formatted_data = {\n",
    "        \"system_prompt\": (\n",
    "            \"You are a Supreme Court Justice participating in oral arguments. \"\n",
    "            \"Given a transcript excerpt and a Justice's name, generate the Justice's next question in response to the conversation history.\"\n",
    "        ),\n",
    "        \"instruction\": (\n",
    "            \"<context>\\n\" +\n",
    "            \"\\n\".join([f\"<turn>{turn['speaker_name']}: {turn['text']}</turn>\" for turn in context_turns]) +\n",
    "            \"\\n</context>\\n\" +\n",
    "            f\"<justice>{justice_name}</justice>\\n\" +\n",
    "            f\"Generate a question that {justice_name} is likely to ask next.\"\n",
    "        ),\n",
    "        \"output\": f\"{justice_name}: {justice_turn['text']}\",\n",
    "        \"transcript_id\": transcript_id,\n",
    "        \"justice\": justice_name,\n",
    "    }\n",
    "    \n",
    "    return formatted_data\n",
    "\n",
    "def process_turns(turn_data, transcript_id, max_context_chars=5000):\n",
    "    '''\n",
    "        Convert list of turns to expected format with a sliding window of max 3 turns\n",
    "    '''\n",
    "    formatted_data_list = []\n",
    "    context_window = []\n",
    "    context_char_count = 0\n",
    "\n",
    "    for i in range(len(turn_data)):  \n",
    "        current_turn = turn_data[i]\n",
    "\n",
    "        # Only add this as sample if the current turn is spoken by a Justice and is not the first turn\n",
    "        if current_turn[\"role\"] == \"scotus_justice\" and len(context_window) > 0:\n",
    "            formatted_data = format_conversation_segment(context_window, current_turn, transcript_id)\n",
    "            formatted_data_list.append(formatted_data)\n",
    "\n",
    "        # Add turn to context\n",
    "        current_turn_text = f\"{current_turn['speaker_name']}: {current_turn['text']}\"\n",
    "        context_window.append(current_turn)\n",
    "        context_char_count += len(current_turn_text)\n",
    "\n",
    "        # Ensure context stays within max_context_chars\n",
    "        while context_char_count > max_context_chars and context_window:\n",
    "            removed_turn = context_window.pop(0)\n",
    "            removed_text = f\"{removed_turn['speaker_name']}: {removed_turn['text']}\"\n",
    "            context_char_count -= len(removed_text)\n",
    "\n",
    "    return formatted_data_list\n",
    "\n",
    "def get_transcript_data(json_file_name):\n",
    "    '''\n",
    "    Parse JSON oral argument transcript into the formatted data needed for finetuning.\n",
    "\n",
    "    @param json_file_name -- name of oral argument JSON file\n",
    "    @return -- list of samples for finetuning\n",
    "    '''\n",
    "\n",
    "    transcript_file_path = TRANSCRIPTS_DIR + json_file_name\n",
    "    with open(transcript_file_path) as json_file:\n",
    "        transcript_json = json.load(json_file)\n",
    "    \n",
    "    transcript_id = json_file_name[:-5]\n",
    "    formatted_data = []\n",
    "\n",
    "    for section in [0, 1]:\n",
    "        section_turns = transcript_json[\"transcript\"][\"sections\"][section][\"turns\"]\n",
    "        section_turns = [get_formatted_text_of_turn(turn) for turn in section_turns]\n",
    "        section_turns = [turn for turn in section_turns if turn]\n",
    "        formatted_data.extend(process_turns(section_turns, transcript_id))\n",
    "\n",
    "    return formatted_data\n",
    "\n",
    "'''\n",
    "Parses then adds all historical transcript data into jsonl file with samples for finetuning\n",
    "'''\n",
    "data_transcripts = []\n",
    "cases_dir = os.fsencode(TRANSCRIPTS_DIR)\n",
    "success = fail = 0\n",
    "for json_file_name in os.listdir(TRANSCRIPTS_DIR):\n",
    "    if json_file_name.endswith('.json'):\n",
    "        data_transcripts.extend(get_transcript_data(json_file_name))\n",
    "\n",
    "\n",
    "# output_file = f\"{OUT_DIR}/finetuning_data_processed.jsonl\"\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     for entry in data_transcripts:\n",
    "#         f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# print(f\"Saved {len(data_transcripts)} fine-tuning examples to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Filter Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples BEFORE filtering: 230841\n",
      "# samples AFTER filtering: 29680\n",
      "# samples by justice\n",
      "Justice John G. Roberts, Jr.     5997\n",
      "Justice Sonia Sotomayor          5931\n",
      "Justice Samuel A. Alito, Jr.     5746\n",
      "Justice Elena Kagan              4122\n",
      "Justice Neil Gorsuch             2432\n",
      "Justice Brett M. Kavanaugh       1736\n",
      "Justice Ketanji Brown Jackson    1358\n",
      "Justice Amy Coney Barrett        1207\n",
      "Justice Clarence Thomas          1151\n",
      "Name: count, dtype: int64\n",
      "Saved filtered dataset to: ../datasets/finetune/finetuning_data_filtered.jsonl\n"
     ]
    }
   ],
   "source": [
    "def filter_justices(sample):\n",
    "    current_justices = {\"Justice John G. Roberts, Jr.\", \"Justice Clarence Thomas\", \"Justice Samuel A. Alito, Jr.\", \"Justice Sonia Sotomayor\", \"Justice Elena Kagan\", \"Justice Neil Gorsuch\", \"Justice Brett M. Kavanaugh\", \"Justice Amy Coney Barrett\", \"Justice Ketanji Brown Jackson\"}\n",
    "    return sample in current_justices\n",
    "\n",
    "def filter_questions(sample):\n",
    "    ''' \n",
    "        Filter out data heuristically: >50 chars and has a '?' char to indicate a justice question.\n",
    "    '''\n",
    "    text = sample.split(': ', 1)[1]\n",
    "    return len(text) > 50 and \"?\" in text and \"inaudible\" not in text.lower()\n",
    "\n",
    "# # Load formatted dataset\n",
    "# input_file = f\"{OUT_DIR}/finetuning_data_processed.jsonl\" \n",
    "# df = pd.read_json(input_file, lines=True) # Load data from saved finetuning_data_processed.jsonl file from previous step\n",
    "df = pd.DataFrame(data_transcripts) # Load data directly from memory\n",
    "\n",
    "print(f\"# samples BEFORE filtering: {len(df)}\")\n",
    "# Filter data\n",
    "df = df[df['output'].apply(filter_questions)]\n",
    "df = df[df['justice'].apply(filter_justices)]\n",
    "print(f\"# samples AFTER filtering: {len(df)}\")\n",
    "print(f\"# samples by {df['justice'].value_counts()}\")\n",
    "\n",
    "# Save filtered dataset\n",
    "# output_file = f\"{OUT_DIR}/finetuning_data_filtered.jsonl\"\n",
    "# save_jsonl(df, output_file)\n",
    "# print(f\"Saved filtered dataset to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete:\n",
      "Train: 23365\n",
      "Validation: 3402\n",
      "Test: 2913\n",
      "\n",
      "TRANSCRIPT ID counts:\n",
      "Total: 1294\n",
      "Train: 1035\n",
      "Validation: 129\n",
      "Test: 130\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "# input_file = f\"{OUT_DIR}/finetuning_data_filtered.jsonl\"\n",
    "# df = pd.read_json(input_file, lines=True) # Load data from saved finetuning_data_filtered.jsonl file from previous step\n",
    "\n",
    "train_ratio = 0.80\n",
    "val_ratio = 0.10\n",
    "test_ratio = 0.10\n",
    "\n",
    "# 0. get unique transcript_ids\n",
    "transcript_ids = df['transcript_id'].unique()\n",
    "\n",
    "# 1. split into train (80%) and eval (20%)\n",
    "train_ids, eval_ids = train_test_split(\n",
    "    transcript_ids, \n",
    "    test_size=(val_ratio + test_ratio), \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# 2. split eval into validation (10%) and test (10%)\n",
    "val_ids, test_ids = train_test_split(\n",
    "    eval_ids, \n",
    "    test_size=(test_ratio / (val_ratio + test_ratio)), \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "train_data = df[df['transcript_id'].isin(train_ids)]\n",
    "val_data = df[df['transcript_id'].isin(val_ids)]\n",
    "test_data = df[df['transcript_id'].isin(test_ids)]\n",
    "\n",
    "# 3. save splits\n",
    "save_jsonl(train_data, f\"{OUT_DIR}/train.jsonl\")\n",
    "save_jsonl(val_data, f\"{OUT_DIR}/val.jsonl\")\n",
    "save_jsonl(test_data, f\"{OUT_DIR}/test.jsonl\")\n",
    "\n",
    "print(f\"Dataset split complete:\\nTrain: {len(train_data)}\\nValidation: {len(val_data)}\\nTest: {len(test_data)}\")\n",
    "print(f\"\\nTRANSCRIPT ID counts:\\nTotal: {len(transcript_ids)}\\nTrain: {len(train_ids)}\\nValidation: {len(val_ids)}\\nTest: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create small test subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "SAMPLE_SIZE = 100\n",
    "\n",
    "input_file = f\"{OUT_DIR}/test.jsonl\"\n",
    "df = pd.read_json(input_file, lines=True)\n",
    "\n",
    "test_sample = df.sample(n=SAMPLE_SIZE, random_state=RANDOM_SEED)\n",
    "save_jsonl(test_sample, f\"{OUT_DIR}/test_{SAMPLE_SIZE}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify proportions of justice and years across splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_proportions(train_data, val_data, test_data, column):\n",
    "    train_proportions = train_data[column].value_counts(normalize=True).sort_index()\n",
    "    val_proportions = val_data[column].value_counts(normalize=True).sort_index()\n",
    "    test_proportions = test_data[column].value_counts(normalize=True).sort_index()\n",
    "\n",
    "    proportion_df = pd.DataFrame({\n",
    "        'Train': train_proportions,\n",
    "        'Validation': val_proportions,\n",
    "        'Test': test_proportions\n",
    "    })\n",
    "    proportion_df = proportion_df * 100\n",
    "    print(proportion_df)\n",
    "    proportion_df.plot(kind='bar', figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUSTICE PROPORTIONS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_proportions(train_data, val_data, test_data, column='justice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YEAR PROPORTIONS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_year_column(dataframe):\n",
    "#     dataframe['year'] = dataframe['transcript_id'].apply(lambda x: x.split('.')[0])\n",
    "#     return dataframe\n",
    "\n",
    "# train_data = add_year_column(train_data)\n",
    "# val_data = add_year_column(val_data)\n",
    "# test_data = add_year_column(test_data)\n",
    "\n",
    "# check_proportions(train_data, val_data, test_data, column='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug Raw data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # debugging data\n",
    "# turns_data = {\n",
    "#     \"turns\": [\n",
    "#         {\n",
    "#             \"speaker_name\": \"MR. McCONNELL\",\n",
    "#             \"role\": \"counsel\",\n",
    "#             \"text\": \"1. Mr. Chief Justice, and may it please the Court to understand...\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"JUSTICE KENNEDY\",\n",
    "#             \"role\": \"scotus_justice\",\n",
    "#             \"text\": \"2. But we do have the problem of the stipulation...\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"MR. McCONNELL\",\n",
    "#             \"role\": \"counsel\",\n",
    "#             \"text\": \"3. Happy to, Justice Kennedy. If you just look with me at Joint Stipulation 17...\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"JUSTICE SCALIA\",\n",
    "#             \"role\": \"scotus_justice\",\n",
    "#             \"text\": \"4. But isn't the all-comers policy broader than the nondiscrimination policy?\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"JUSTICE KENNEDY\",\n",
    "#             \"role\": \"scotus_justice\",\n",
    "#             \"text\": \"5. But we do have the problem of the stipulation...\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"JUSTICE SCALIA\",\n",
    "#             \"role\": \"scotus_justice\",\n",
    "#             \"text\": \"6. But isn't the all-comers policy broader than the nondiscrimination policy?\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"MR. McCONNELL\",\n",
    "#             \"role\": \"counsel\",\n",
    "#             \"text\": \"7. TAKE 3 Happy to, Justice Kennedy. If you just look with me at Joint Stipulation 17...\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"JUSTICE SCALIA\",\n",
    "#             \"role\": \"scotus_justice\",\n",
    "#             \"text\": \"8. ROUND @ But isn't the all-comers policy broader than the nondiscrimination policy?\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"MR. McCONNELL\",\n",
    "#             \"role\": \"counsel\",\n",
    "#             \"text\": \"9. TAKE 4 Happy to, Justice Kennedy. If you just look with me at Joint Stipulation 17...\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"JUSTICE KENNEDY\",\n",
    "#             \"role\": \"scotus_justice\",\n",
    "#             \"text\": \"10. But we do have the problem of the stipulation...\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"JUSTICE SCALIA\",\n",
    "#             \"role\": \"scotus_justice\",\n",
    "#             \"text\": \"11. But isn't the all-comers policy broader than the nondiscrimination policy?\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"speaker_name\": \"MR. McCONNELL\",\n",
    "#             \"role\": \"counsel\",\n",
    "#             \"text\": \"12. Happy to, Justice Kennedy. If you just look with me at Joint Stipulation 17...\"\n",
    "#         },\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# MAX_CONTEXT_CHARS = 500\n",
    "\n",
    "# formatted_data_list = []\n",
    "# context_window = []\n",
    "# context_char_count = 0\n",
    "\n",
    "# for i in range(len(turns_data)):  \n",
    "#     current_turn = turns_data[i]\n",
    "\n",
    "#     if current_turn[\"role\"] == \"scotus_justice\":\n",
    "#         formatted_data = format_conversation_segment(context_window, current_turn, 'debug_transcript_id')\n",
    "#         formatted_data_list.append(formatted_data)\n",
    "\n",
    "#     current_turn_text = f\"{current_turn['speaker_name']}: {current_turn['text']}\"\n",
    "#     context_window.append(current_turn)\n",
    "#     context_char_count += len(current_turn_text)\n",
    "\n",
    "#     while context_char_count > MAX_CONTEXT_CHARS and context_window:\n",
    "#         removed_turn = context_window.pop(0)\n",
    "#         removed_text = f\"{removed_turn['speaker_name']}: {removed_turn['text']}\"\n",
    "#         context_char_count -= len(removed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(formatted_data_list)):\n",
    "#     print(formatted_data_list[i]['messages'][1]['content'])\n",
    "#     print(formatted_data_list[i]['messages'][2]['content'])\n",
    "#     print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
