{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23693d69-b821-48cd-8eaa-e5239499d70c",
   "metadata": {},
   "source": [
    "## Convert OS based questions to DIALOGUE STYLE questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed7c50-662b-4587-aa88-4c02be83c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db97a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIR = \"../../datasets/original\"\n",
    "OUT_DIR = \"../finetuning_datasets/eval_only\"\n",
    "\n",
    "TRANSCRIPTS_DIR = \"../../2024_cases_json/\"\n",
    "CASEBRIEF_DIR = \"../../2023-2024_case_briefs/\"      # directory of raw JSONs of case briefs\n",
    "\n",
    "def save_jsonl(df, filename):\n",
    "    df.to_json(filename, orient=\"records\", lines=True)\n",
    "\n",
    "def read_jsonl(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f29a1",
   "metadata": {},
   "source": [
    "## Generate jsonl for inference on finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed742822",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp = f'{IN_DIR}/2024_all_questions.csv'\n",
    "questions_df = pd.read_csv(input_fp)\n",
    "justices = list(questions_df['justice'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "justices = [\n",
    "    \"justice_amy_coney_barrett\",\n",
    "    \"justice_brett_m_kavanaugh\",\n",
    "    \"justice_clarence_thomas\",\n",
    "    \"justice_elena_kagan\",\n",
    "    \"justice_john_g_roberts_jr\",\n",
    "    \"justice_ketanji_brown_jackson\",\n",
    "    \"justice_neil_gorsuch\",\n",
    "    \"justice_samuel_a_alito_jr\",\n",
    "    \"justice_sonia_sotomayor\"\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp = f'{IN_DIR}/2024_full_text_transcripts.csv'\n",
    "df = pd.read_csv(input_fp)\n",
    "\n",
    "\n",
    "def extract_speaker_and_text(input_string):\n",
    "    speaker_pattern = re.compile(r\"<speaker>(.*?)</speaker>\", re.DOTALL)\n",
    "    text_pattern = re.compile(r\"<text>(.*?)</text>\", re.DOTALL)\n",
    "\n",
    "    speaker_match = speaker_pattern.search(input_string)\n",
    "    text_match = text_pattern.search(input_string)\n",
    "\n",
    "    speaker = speaker_match.group(1)\n",
    "    text_content = text_match.group(1)\n",
    "\n",
    "    # turn = f\"{speaker}: {text_content}\"\n",
    "    turn = text_content\n",
    "\n",
    "    return turn\n",
    "\n",
    "df['petitioner_turn'] = df['petitioner_opening_text'].apply(extract_speaker_and_text)\n",
    "df['respondent_turn'] = df['respondent_opening_statement'].apply(extract_speaker_and_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02dd4b2-f6c4-413e-9e29-38c31faa4393",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc74fdad-7583-45db-86b8-7f43774b36df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    if text:\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        # Replace unicode characters\n",
    "        text = text.replace(\"\\u201c\", \"\\\"\").replace(\"\\u201d\", \"\\\"\").replace(\"\\u2018\", \"'\").replace(\"\\u2019\", \"'\")\n",
    "        return text.strip()\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "def get_facts_and_question(transcript_id, dir=CASEBRIEF_DIR):\n",
    "    case_brief_file_path = os.path.join(dir, transcript_id + \".json\")\n",
    "    with open(case_brief_file_path, 'r') as json_file:\n",
    "        case_brief_json = json.load(json_file)\n",
    "        facts = clean_text(case_brief_json['facts_of_the_case'])\n",
    "        question = clean_text(case_brief_json['question'])\n",
    "        return facts, question\n",
    "\n",
    "def get_system_prompt(transcript_id):\n",
    "    facts, question = get_facts_and_question(transcript_id)\n",
    "    return f\"You are a legal expert trained to simulate Supreme Court oral arguments.\\n\\nFACTS_OF_THE_CASE:\\n{facts}\\n\\nLEGAL_QUESTION:\\n{question}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab461d46-b29f-4a73-9321-8b4011e80d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_formatted_text_of_turn(turn, advocate):\n",
    "    '''\n",
    "    Return all text within a turn as a dict denoting speaker role, and text.\n",
    "\n",
    "    @param turn -- JSON representing a single speaker turn\n",
    "    @return -- Dict with keys \"role\", \"content\"\n",
    "    '''\n",
    "    if not turn[\"speaker\"]:  # skip turns that have no speaker like \"Laughter\"\n",
    "        return None\n",
    "\n",
    "    if not turn[\"speaker\"][\"roles\"]:\n",
    "        role = \"attorney\"\n",
    "    elif ('2' in turn[\"speaker\"][\"roles\"] and turn[\"speaker\"][\"roles\"]['2'][\"type\"] == \"scotus_justice\") or \\\n",
    "         turn[\"speaker\"][\"roles\"][0][\"type\"] == \"scotus_justice\":\n",
    "        role = \"scotus_justice\"\n",
    "\n",
    "    if role == \"scotus_justice\":\n",
    "        identifier = f'justice_{turn[\"speaker\"][\"identifier\"]}'\n",
    "    else:\n",
    "        identifier = advocate\n",
    "\n",
    "    text = \" \".join([block[\"text\"] for block in turn[\"text_blocks\"]])\n",
    "\n",
    "    return {\n",
    "        \"role\": identifier,\n",
    "        \"content\": text\n",
    "    }\n",
    "\n",
    "def get_transcript_data(json_file_name):\n",
    "    '''\n",
    "    @param json_file_name -- Name of the oral argument JSON file\n",
    "    @return -- List of dicts with keys \"role\", \"content\" representing each speaker turn in the transcript\n",
    "    '''\n",
    "\n",
    "    transcript_file_path = os.path.join(TRANSCRIPTS_DIR, json_file_name)\n",
    "    with open(transcript_file_path, 'r') as json_file:\n",
    "        transcript_json = json.load(json_file)\n",
    "\n",
    "    formatted_turns = []\n",
    "    for section in [0, 1]:\n",
    "        advocate = 'respondent' if section else 'petitioner' \n",
    "        section_turns = transcript_json[\"transcript\"][\"sections\"][section][\"turns\"]\n",
    "        section_turns = [get_formatted_text_of_turn(turn, advocate) for turn in section_turns]\n",
    "        section_turns = [turn for turn in section_turns if turn]  # remove None values\n",
    "        formatted_turns.extend(section_turns)\n",
    "\n",
    "    return formatted_turns\n",
    "\n",
    "# Load all transcripts\n",
    "data_transcripts = []\n",
    "cases_dir = os.fsencode(TRANSCRIPTS_DIR)\n",
    "for json_file_name in os.listdir(TRANSCRIPTS_DIR):\n",
    "    if json_file_name.endswith('.json'):\n",
    "        # Extract the transcript_id\n",
    "        transcript_id = json_file_name[:-9].strip()\n",
    "        try:\n",
    "            # Load the corresponding case brief and extract the facts of the case and the legal question\n",
    "            system_prompt = get_system_prompt(transcript_id)\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt\n",
    "                }\n",
    "            ]\n",
    "        except Exception:\n",
    "            print(f\"Could not get facts and question from case brief: Skipping {transcript_id}\")\n",
    "            continue\n",
    "        # Load the transcript and extract the messages\n",
    "        messages.extend(get_transcript_data(json_file_name))\n",
    "        data_transcripts.append({\n",
    "            \"transcript_id\": transcript_id,\n",
    "            \"messages\": messages\n",
    "        })\n",
    "\n",
    "dialogues = [transcript[\"messages\"] for transcript in data_transcripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce2ce950-8588-4fc5-8fbb-c34bfac988df",
   "metadata": {},
   "outputs": [],
   "source": [
    "petitioner_chat = [d[:3] for d in dialogues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a41a66d-1305-46b8-85cf-1e92847a9ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(petitioner_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6c25bb-e50e-4032-b543-ac33ed420039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nnadeem/.conda/envs/llama_finetuning_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/nnadeem/.conda/envs/llama_finetuning_env/lib/python3.11/site-packages/transformers/quantizers/auto.py:195: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from peft import PeftModel\n",
    "\n",
    "# base_model_id = \"path/to/original_base_model\" \n",
    "# lora_model_id = \"path/to/finetuned_adapter_folder\"\n",
    "base_model_dir = \"/scratch/gpfs/nnadeem/transformer_cache/Meta-Llama-3.1-8B-Instruct-bnb-4bit/\"\n",
    "adapter_dir = \"../models/finetuned_Meta-Llama-3.1-8B-Instruct-bnb-4bit_dialogue_style/checkpoint-242\"\n",
    "\n",
    "# 1) Load tokenizer from the *base* model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir, use_fast=False)\n",
    "\n",
    "# 2) Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_dir,\n",
    "    # If you are using 4-bit or 8-bit, set it up here\n",
    "    load_in_4bit=True,  \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3) Load the LoRA adapter on top of the base model\n",
    "model = PeftModel.from_pretrained(base_model, adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ecdee7b-470e-4741-b1b8-522518f5823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_chat_template():\n",
    "    return \"\"\"<|begin_of_text|>{%- for message in messages %}<|start_header_id|>{{ message['role'] }}<|end_header_id|>\\n\\n{{ message['content'] }}<|eot_id|>{%- endfor %}\"\"\"\n",
    "tokenizer.chat_template = set_chat_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef5adc",
   "metadata": {},
   "source": [
    "**Sanity Test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd4044b7-a62c-4616-8475-b30723756f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a legal expert trained to simulate Supreme Court oral arguments.\n",
      "\n",
      "FACTS_OF_THE_CASE:\n",
      "ATF, created in 1972, is responsible for regulating firearms under the Gun Control Act of 1968 (GCA). The GCA requires federal firearms licensees (FFLs) to conduct background checks, record firearm transfers, and serialize firearms when selling or transferring them. The GCA's regulation of firearms is based on the definition of \"firearm,\" which includes the \"frame or receiver.\" However, ATF's 1978 definition of \"frame or receiver\" became outdated due to changes in modern firearm design, such as the AR-15 and Glock pistols. Furthermore, the rise of privately made firearms (PMFs) or \"ghost guns\" posed challenges to law enforcement because they were not regulated under the GCA and did not require serialization. In response, ATF issued a Final Rule in 2022, updating the definitions of \"frame,\" \"receiver,\" and \"firearm\" to better capture modern firearm designs and regulate PMFs. The Final Rule took effect on August 24, 2022.\n",
      "The respondents in this case challenged the Final Rule's redefinition of \"frame or receiver\" and \"firearm,\" arguing that it exceeded ATF's congressionally mandated authority. The district court granted summary judgment to the plaintiffs and vacated the Final Rule in its entirety. The U.S. Court of Appeals for the Fifth Circuit affirmed the district court's determination that the two provisions exceeded ATF's statutory authority.\n",
      "\n",
      "LEGAL_QUESTION:\n",
      "Did the ATF exceed its statutory authority in promulgating its Final Rule purporting to regulate so-called \"ghost guns\"?<|eot_id|><|start_header_id|>justice_john_g_roberts_jr<|end_header_id|>\n",
      "\n",
      " We will hear argument first this morning in Case 23-852, Garland versus VanDerStok. General Prelogar.<|eot_id|><|start_header_id|>petitioner<|end_header_id|>\n",
      "\n",
      " Mr. Chief Justice, and may it please the Court:  The Gun Control Act imposes straightforward but essential requirements. Firearms sellers and manufacturers must mark their products with serial numbers, maintain sales records, and conduct background checks. The industry has followed those conditions without difficulty for more than half a century, and those basic requirements are crucial to solving gun crimes and keeping guns out of the hands of minors, felons, and domestic abusers. But, in recent years, companies like the Respondents here have tried to circumvent those requirements. They've begun selling firearms as easy -to-assemble kits and frames and receivers that require minimal work to be made functional. They've advertised the products, in their words, as \"ridiculously easy to assemble and dummy-proof\" and touted that you can go from opening the mail to have a fully functional gun  in as little as 15 minutes, no serial number, background check, or records required. Those untraceable guns are attractive to people who can't lawfully purchase them or  who plan to use them in crimes. As a result, our nation has seen an explosion in crimes committed with ghost guns. In the face of that public safety crisis, ATF promulgated this rule to underscore two points about the Gun Control Act's plain text. First, a weapon parts kit that can readily be converted to function as a gun with common tools, often in under an hour, is a covered firearm. Second, a product is a frame or receiver under the Act even if the buyer must drill a few holes or remove a few superfluous pieces of plastic to make it functional. Both of those points are consistent with how ATF has interpreted and implemented the Act across five decades and 11 different presidential administrations. Respondents now seek a sea change in the Act's scope. They claim that if a firearm isn't a hundred percent functional, if it's  missing just one hole that could be drilled in seconds and immediately assembled into a working gun, that product can be sold to anyone online  with no background check, no records, and no serial number. That contradicts the Act's plain text, and it also contradicts common sense. This Court should make clear that the Act regulates these products as what they are, firearms and frames and receivers of firearms. I welcome the Court's questions.<|eot_id|><|start_header_id|>justice_sonia_sotomayor<|end_header_id|>\n",
      "\n",
      " Mr. Prelogar, can you tell me what the difference is between a frame and a receiver?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "formatted_chats = [tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=False) for sample in petitioner_chat]\n",
    "prompt = formatted_chats[0]\n",
    "justice = \"justice_sonia_sotomayor\"\n",
    "prompt += f\"<|start_header_id|>{justice}<|end_header_id|>\\n\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4baad27",
   "metadata": {},
   "source": [
    "**Test regex for extracting question from generation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37ce8f4d-4a1b-4ce6-885a-bfed0b818071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mr. Prelogar, can you tell me what the difference is between a frame and a receiver?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_justice_text(transcript: str, justice_identifier: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the text between <|start_header_id|>justice_identifier<|end_header_id|>\n",
    "    and .<|eot_id|> from the transcript.\n",
    "    \n",
    "    Returns the matched text (with surrounding whitespace stripped),\n",
    "    or None if no match is found.\n",
    "    \"\"\"\n",
    "    # Build a pattern specific to the provided justice identifier\n",
    "    pattern = (\n",
    "        rf\"<\\|start_header_id\\|>{justice_identifier}<\\|end_header_id\\|>\"  # Match the start marker\n",
    "        r\"(.*?)\"                                                         # Captures everything (non-greedy)\n",
    "        r\"<\\|eot_id\\|>\"                                               # Until a period + <|eot_id|>\n",
    "    )\n",
    "    \n",
    "    match = re.search(pattern, transcript, flags=re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "full_response = tokenizer.decode(outputs[0])\n",
    "j = \"justice_sonia_sotomayor\"\n",
    "extract_justice_text(full_response, j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80d293d",
   "metadata": {},
   "source": [
    "**Try full flow**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff27d9-da10-4651-9955-4f53ba1d4ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transcript 0...\n",
      "\n",
      "\n",
      "Processing justice justice_amy_coney_barrett...\n",
      "Processing justice justice_brett_m_kavanaugh...\n",
      "Processing justice justice_clarence_thomas...\n"
     ]
    }
   ],
   "source": [
    "formatted_chats = [tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=False) for sample in petitioner_chat]\n",
    "results = []\n",
    "for i, chat in enumerate(formatted_chats):\n",
    "    print(f\"Processing transcript {i}...\\n\\n\")\n",
    "    result = {\n",
    "        \"prompt\": chat,\n",
    "        \"responses\": []\n",
    "    }\n",
    "    for j in justices:\n",
    "        print(f\"Processing justice {j}...\")\n",
    "        chat += f\"<|start_header_id|>{j}<|end_header_id|>\\n\\n\"\n",
    "        inputs = tokenizer(chat, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs)\n",
    "        full_response = tokenizer.decode(outputs[0])\n",
    "        result[\"responses\"].append({\n",
    "            \"justice\": j,\n",
    "            \"full_response\": full_response,\n",
    "            \"parsed_response\": extract_justice_text(full_response, j)\n",
    "        })\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43de47-d821-4331-8f23-b19d7baf83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][\"responses\"][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_finetuning_env [~/.conda/envs/llama_finetuning_env/]",
   "language": "python",
   "name": "conda_llama_finetuning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
