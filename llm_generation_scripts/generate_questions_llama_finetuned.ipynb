{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23693d69-b821-48cd-8eaa-e5239499d70c",
   "metadata": {},
   "source": [
    "## Create Generate Questions jsonl for finetuned model [ROUGH RN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b984023",
   "metadata": {},
   "source": [
    "** Rough Notebook rn, needs cleanup **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ed7c50-662b-4587-aa88-4c02be83c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce9f408-b45f-4931-ad60-6742bf290a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"http://localhost:12257/v1/chat/completions\"\n",
    "\n",
    "# headers={\n",
    "#     \"Content-Type\": \"application/json\",\n",
    "#     \"Authorization\": \"token-abc123\"\n",
    "# }\n",
    "\n",
    "# model_name = 'meta-llama/Meta-Llama-3.1-70B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a20db-ba35-4254-b174-e376059bd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_generation_prompt(justice_name, opening_statement):\n",
    "    #     system_prompt = \"\"\"Below is an opening statement from a Supreme Court case and a question asked during the argument. Your task is to determine whether the question logically follows from the opening statement alone.\n",
    "\n",
    "    #     Repond with a only a single word. If the question makes sense based solely on the content of the opening statement, label it as \"coherent.\" If additional context from other parts of the argument is needed for the question to make sense, label it as \"incoherent.\"\n",
    "    #     \"\"\"\n",
    "    #     user_prompt = f\"\"\"\n",
    "    #     ### Opening Statement:\n",
    "    #     {opening_statement}\n",
    "\n",
    "    #     ### Questions:\n",
    "    #     \"\"\"\n",
    "    # system_prompt = \"\"\"You are a legal expert trained to simulate the questioning style of Supreme Court justices during oral arguments. Below is an opening statement from a Supreme Court case and the name of a specific justice. Your task is to generate a list of questions that this justice is likely to ask in response to the opening statement. These questions should reflect the justice’s known priorities, jurisprudence, and typical questioning style, and they should be directly relevant to the arguments presented in the opening statement.\n",
    "\n",
    "    #     ### Instructions:\n",
    "    #     1. Carefully analyze the opening statement to identify key arguments, assumptions, or ambiguities.\n",
    "    #     2. Generate questions that the specified justice might ask. Limit to 15 or less questions. Do not repeat questions.\n",
    "    #     3. Format each question with specific start and end tags for easier parsing:\n",
    "    #        - Use `<START_Q>` at the beginning of each question.\n",
    "    #        - Use `<END_Q>` at the end of each question.\n",
    "\n",
    "    #     ### Example:\n",
    "    #     Justice: Justice Oliver Wendell Holmes Jr.\n",
    "    #     Opening Statement: Congress intended a preponderance of the evidence standard to apply to FLSA exemptions. Respondents argue for a clear and convincing evidence standard due to the importance of overtime rights. However, waivability of rights and standards of proof are unrelated.\n",
    "\n",
    "    #     Generated Questions:\n",
    "    #     <START_Q> Is there any explicit legislative language that supports your claim regarding the preponderance standard? <END_Q>\n",
    "    #     <START_Q> How does the principle of statutory construction guide us in resolving ambiguities in this case? <END_Q>\n",
    "    #     <START_Q> Would adopting a clear and convincing standard conflict with the broader purpose of the Fair Labor Standards Act? <END_Q>\n",
    "    #     <START_Q> How do you reconcile your argument with precedents where the Court imposed heightened standards for \"important\" rights? <END_Q>\n",
    "\n",
    "    #     ### Your Task:\n",
    "    #     Justice: {justice_name}\n",
    "    #     Opening Statement: {opening_statement}\n",
    "\n",
    "    #     ### Output:\n",
    "    #     A list of questions that {justice_name} is likely to ask:\n",
    "    #     <START_Q> text of question one <END_Q>\n",
    "    #     <START_Q> text of question two <END_Q>\n",
    "    #     <START_Q> text of question three <END_Q>\n",
    "\n",
    "    # \"\"\"\n",
    "    \n",
    "    # user_prompt = f\"\"\"### Your Task:\n",
    "    #     Justice: {justice_name}\n",
    "    #     Opening Statement: {opening_statement}\n",
    "\n",
    "    #     ### Output:\n",
    "    # \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\"You are a legal expert trained to simulate the questioning style of Supreme Court justices during oral arguments. Below is an opening statement from a Supreme Court case and the name of a specific justice. Your task is to generate a list of questions that this justice is likely to ask in response to the opening statement. These questions should reflect the justice’s known priorities, jurisprudence, and typical questioning style, and they should be directly relevant to the arguments presented in the opening statement.\n",
    "\n",
    "        ### Instructions:\n",
    "        1. Carefully analyze the opening statement to identify key arguments, assumptions, or ambiguities.\n",
    "        2. Generate questions that the specified justice might ask. Limit to 15 or less questions. Do not repeat questions.\n",
    "        3. Format each question with specific start and end tags for easier parsing:\n",
    "           - Use `<START_Q>` at the beginning of each question.\n",
    "           - Use `<END_Q>` at the end of each question.\n",
    "\n",
    "        ### Example:\n",
    "        ### Your Task:\n",
    "        Justice: {justice_name}\n",
    "        Opening Statement: {opening_statement}\n",
    "\n",
    "        ### Output:\n",
    "        A list of questions that {justice_name} is likely to ask:\n",
    "        <START_Q> text of question one <END_Q>\n",
    "        <START_Q> text of question two <END_Q>\n",
    "        <START_Q> text of question three <END_Q>\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"### Your Task:\n",
    "        Justice: {justice_name}\n",
    "        Opening Statement: {opening_statement}\n",
    "\n",
    "        ### Output:\n",
    "    \"\"\"\n",
    "\n",
    "    # system_prompt = \"\"\"You are a legal expert trained to simulate the questioning style of Supreme Court justices during oral arguments.\n",
    "    # \"\"\"\n",
    "    \n",
    "    # user_prompt = f\"\"\"Below is an opening statement from a Supreme Court case and the name of a specific justice. Your task is to generate a list of questions that this justice is likely to ask in response to the opening statement. These questions should reflect the justice’s known priorities, jurisprudence, and typical questioning style, and they should be directly relevant to the arguments presented in the opening statement.\n",
    "\n",
    "    #     ### Instructions:\n",
    "    #     1. Carefully analyze the opening statement to identify key arguments, assumptions, or ambiguities.\n",
    "    #     2. Generate questions that the specified justice might ask. Limit to 15 or less questions. Do not repeat questions.\n",
    "    #     3. Format each question with specific start and end tags for easier parsing:\n",
    "    #        - Use `<START_Q>` at the beginning of each question.\n",
    "    #        - Use `<END_Q>` at the end of each question.\n",
    "\n",
    "    #     ### Example:\n",
    "    #     Justice: Justice Oliver Wendell Holmes Jr.\n",
    "    #     Opening Statement: Congress intended a preponderance of the evidence standard to apply to FLSA exemptions. Respondents argue for a clear and convincing evidence standard due to the importance of overtime rights. However, waivability of rights and standards of proof are unrelated.\n",
    "\n",
    "    #     Generated Questions:\n",
    "    #     <START_Q> Is there any explicit legislative language that supports your claim regarding the preponderance standard? <END_Q>\n",
    "    #     <START_Q> How does the principle of statutory construction guide us in resolving ambiguities in this case? <END_Q>\n",
    "    #     <START_Q> Would adopting a clear and convincing standard conflict with the broader purpose of the Fair Labor Standards Act? <END_Q>\n",
    "    #     <START_Q> How do you reconcile your argument with precedents where the Court imposed heightened standards for \"important\" rights? <END_Q>\n",
    "\n",
    "    #     ### Your Task:\n",
    "    #     Justice: {justice_name}\n",
    "    #     Opening Statement: {opening_statement}\n",
    "\n",
    "    #     ### Output:\n",
    "    #     A list of questions that {justice_name} is likely to ask:\n",
    "    #     <START_Q> text of question one <END_Q>\n",
    "    #     <START_Q> text of question two <END_Q>\n",
    "    #     <START_Q> text of question three <END_Q>\n",
    "    \n",
    "    #     ### Your Task:\n",
    "    #         Justice: {justice_name}\n",
    "    #         Opening Statement: {opening_statement}\n",
    "\n",
    "    #     ### Output:\n",
    "    # \"\"\"\n",
    "\n",
    "    # messages = [\n",
    "    #         {\n",
    "    #             \"role\": \"system\",\n",
    "    #             \"content\": system_prompt,\n",
    "    #         },\n",
    "    #         {\"role\": \"user\", \"content\": user_prompt}\n",
    "    #     ]\n",
    "    messages = {\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"instruction\": user_prompt\n",
    "            }\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28dba16f-6115-4450-8593-0c752d017240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model_response(messages):\n",
    "\n",
    "#     payload = {\n",
    "#         \"model\": model_name,\n",
    "#         \"messages\": messages\n",
    "#     }\n",
    "\n",
    "#     response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "#     return response\n",
    "\n",
    "# def parse_response(response):\n",
    "#     decoded = response.content.decode('utf-8')\n",
    "#     response_data = json.loads(decoded)\n",
    "#     content = response_data['choices'][0]['message']['content']\n",
    "    \n",
    "#     questions = re.findall(r\"<START_Q>(.*?)<END_Q>\", content, re.DOTALL)\n",
    "#     cleaned_questions = [q.strip() for q in questions]\n",
    "\n",
    "#     return cleaned_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57f29a1",
   "metadata": {},
   "source": [
    "## Generate jsonl for inference on finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed742822",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp = '../datasets/original/2024_all_questions.csv'\n",
    "questions_df = pd.read_csv(input_fp)\n",
    "justices = list(questions_df['justice'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32e710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "justices = [\n",
    "    # 'Clarence Thomas',\n",
    "    # 'John G. Roberts, Jr.',\n",
    "    # 'Elena Kagan',\n",
    "    # 'Ketanji Brown Jackson',\n",
    "    'Sonia Sotomayor',\n",
    "    'Samuel A. Alito, Jr.',\n",
    "    # 'Amy Coney Barrett',\n",
    "    # 'Neil Gorsuch',\n",
    "    # 'Brett M. Kavanaugh'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd3cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp = '../datasets/original/2024_full_text_transcripts.csv'\n",
    "df = pd.read_csv(input_fp)\n",
    "data = []\n",
    "\n",
    "for j in justices:\n",
    "    justice = f'Justice {j}'\n",
    "    for _, row in df.iterrows():\n",
    "        # petitioner opening statement \n",
    "        sample = get_question_generation_prompt(justice, row['petitioner_opening_text'])\n",
    "        sample.update({\n",
    "            'justice': justice,\n",
    "            'question_addressee': 'petitioner',\n",
    "            'opening_statement': row['petitioner_opening_text']\n",
    "        })\n",
    "        data.append(sample)\n",
    "\n",
    "        # respondent opening statement \n",
    "        sample = get_question_generation_prompt(justice, row['respondent_opening_statement'])\n",
    "        sample.update({\n",
    "            'justice': justice,\n",
    "            'question_addressee': 'respondent',\n",
    "            'opening_statement': row['respondent_opening_statement']\n",
    "        })\n",
    "        data.append(sample)\n",
    "\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df.to_json('../datasets/finetune/OS_based_questions_test_sotomayor_alito.jsonl', orient='records', index=False, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abec8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e72c321-3461-448a-92e9-5faa5d02c489",
   "metadata": {},
   "source": [
    "#### Read csv with dataframe and try call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233eea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp = '../datasets/original/2024_full_text_transcripts.csv'\n",
    "df = pd.read_csv(input_fp)\n",
    "data = []\n",
    "justice = 'Justice Sonia Sotomayor'\n",
    "for _, row in df.iterrows():\n",
    "    sample = get_question_generation_prompt(justice, row['petitioner_opening_text'])\n",
    "    sample.update({\n",
    "        'justice': justice,\n",
    "        'question_addressee': 'petitioner',\n",
    "        'opening_statement': row['petitioner_opening_text']\n",
    "    })\n",
    "    data.append(sample)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    sample = get_question_generation_prompt(justice, row['respondent_opening_statement'])\n",
    "    sample.update({\n",
    "        'justice': justice,\n",
    "        'question_addressee': 'respondent',\n",
    "        'opening_statement': row['respondent_opening_statement']\n",
    "    })\n",
    "    data.append(sample)\n",
    "\n",
    "justice = 'Justice Samuel A. Alito'\n",
    "for _, row in df.iterrows():\n",
    "    sample = get_question_generation_prompt(justice, row['petitioner_opening_text'])\n",
    "    sample.update({\n",
    "        'justice': justice,\n",
    "        'question_addressee': 'petitioner',\n",
    "        'opening_statement': row['petitioner_opening_text']\n",
    "    })\n",
    "    data.append(sample)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    sample = get_question_generation_prompt(justice, row['respondent_opening_statement'])\n",
    "    sample.update({\n",
    "        'justice': justice,\n",
    "        'question_addressee': 'respondent',\n",
    "        'opening_statement': row['respondent_opening_statement']\n",
    "    })\n",
    "    data.append(sample)\n",
    "\n",
    "data_df = pd.DataFrame(data)\n",
    "data_df.to_json('finetune_question_test.jsonl', orient='records', index=False, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f0ed9-4fa3-464a-ab18-0499b7a18e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fp = '../datasets/original/2024_full_text_transcripts.csv'\n",
    "df = pd.read_csv(input_fp)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a4ef5-196f-427e-8411-16137bcef85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f4f40-898f-4c4c-9ad1-81902c24b5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_justice_questions(justice_name, opening_statement):\n",
    "    messages = get_question_generation_prompt(justice_name, opening_statement)\n",
    "    response = get_model_response(messages)\n",
    "    questions = parse_response(response)\n",
    "    # return json string of list of questions to store in pandas df\n",
    "    return json.dumps(questions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af5f09-9680-4c9c-889e-6d844102efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test on subsample\n",
    "# df_new = df.head(2).copy()\n",
    "# df_new\n",
    "\n",
    "# # add sotomayor, petitioner\n",
    "# justice = 'Justice Sonia Sotomayor'\n",
    "# df_new['questions_sotomayor_petitioner'] = df_new.apply(\n",
    "#     lambda row: add_justice_questions(justice, row['petitioner_opening_text']), axis=1\n",
    "# )\n",
    "# df_new\n",
    "\n",
    "# for qs in df_new['questions_sotomayor_petitioner'].to_list():\n",
    "#     questions = json.loads(qs)\n",
    "#     for q in questions:\n",
    "#         print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0395b-1faa-4c4d-ac2b-74cab6f2c07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE FOR ALL\n",
    "# add sotomayor, petitioner\n",
    "justice = 'Justice Sonia Sotomayor'\n",
    "df['questions_sotomayor_petitioner'] = df.apply(\n",
    "    lambda row: add_justice_questions(justice, row['petitioner_opening_text']), axis=1\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229c84b-b24d-4823-a7b0-d6a0f5b50571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add sotomayor, respondent\n",
    "justice = 'Justice Sonia Sotomayor'\n",
    "df['questions_sotomayor_respondent'] = df.apply(\n",
    "    lambda row: add_justice_questions(justice, row['respondent_opening_statement']), axis=1\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72e9f4-c332-4273-8dcc-7e76e821c41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save mid way in case notebook crashes\n",
    "model_suffix = model_name.split('/')[1]\n",
    "out_dir = '../datasets/llm_outputs/generate_questions'\n",
    "out_fp = f'{out_dir}/2024_full_text_sotomayor_questions_{model_suffix}.csv'\n",
    "df.to_csv(out_fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29230222-d058-432c-90df-8dc552fc961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add alito, respondent\n",
    "justice = 'Justice Samuel A. Alito'\n",
    "df['questions_alito_respondent'] = df.apply(\n",
    "    lambda row: add_justice_questions(justice, row['respondent_opening_statement']), axis=1\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351f5b4-84c7-423a-b4cf-1081799edd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE FOR ALL\n",
    "# add alito, petitioner\n",
    "justice = 'Justice Samuel A. Alito'\n",
    "df['questions_alito_petitioner'] = df.apply(\n",
    "    lambda row: add_justice_questions(justice, row['petitioner_opening_text']), axis=1\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18fc39-faf8-49d1-ac2b-af0d6ad43e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final df\n",
    "model_suffix = model_name.split('/')[1]\n",
    "out_dir = '../datasets/llm_outputs/generate_questions'\n",
    "out_fp = f'{out_dir}/2024_full_text_sotomayor_alito_questions_{model_suffix}.csv'\n",
    "df.to_csv(out_fp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a88098a-506b-4a95-a64c-9897664eb54f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Try sample call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a2a10-904d-4246-bcfb-a1166a767705",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_os = df['petitioner_opening_text'][0]\n",
    "sample_os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37731ed1-8dd3-4650-96fb-4bee1295bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "justice = 'Justice Sonia Sotomayor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906b82c-d1b6-466f-9f35-297870e7dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = get_question_generation_prompt(justice, sample_os)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dabde6-1dc5-46a6-958d-bf5c3e926188",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_model_response(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a678a-c444-485f-b724-8a385cf77754",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = response.content.decode('utf-8')\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10386aeb-3327-44c4-b809-f0f883e037be",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = parse_response(response)\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48ef88-352c-498b-a487-0616f79fdf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(questions)\n",
    "for q in questions:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ad904-df7a-4a66-af34-31566b41557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_str = json.dumps(questions)\n",
    "qs_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d19f6-ba24-496f-a927-de7159857c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_orig = json.loads(qs_str)\n",
    "qs_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c497d-b78d-44cd-a45e-73a319037e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(questions)\n",
    "for q in questions:\n",
    "    print(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
